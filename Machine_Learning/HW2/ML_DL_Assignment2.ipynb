{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bhfIKbQzGq2"
      },
      "source": [
        "# Assignment 2. Music Century Classification\n",
        "#### Roey Graif 315111401\n",
        "For this task, we will construct models to predict the century in which a music piece was released. We will utilize the \"YearPredictionMSD Data Set,\" which is derived from the Million Song Dataset from the UCI Machine Learning Repository. Make sure you download the version of the dataset from the moodle and not from UCI. Here are some relevant links to read on this dataset:\n",
        "\n",
        "- https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n",
        "- http://millionsongdataset.com/pages/tasks-demos/#yearrecognition\n",
        "\n",
        "Just like in the last assignment, it is divided to two files.\n",
        "1. This file (ML_DL_Assignment2.ipynb)\n",
        "2. A python functions  file which you will fill out (ML_DL_Functions2.py)\n",
        "\n",
        "As well as the year prediction msd dataset file.\n",
        "\n",
        "In this assignment you will mount and load the dataset and functions file from google drive. To start make sure you have both the template python functions file and the song dataset file(downloaded from the moodle) on the same directory in your google drive.\n",
        "\n",
        "When you are finished with the assignment make sure you submit the following files:\n",
        "1. this file (ML_DL_Assignment2.ipynb).\n",
        "2. the functions file (ML_DL_Functions2.py).\n",
        "3. the weights file from section 2.7 (assignment2_submission_optimal_weights.npy).\n",
        "4. the bias file from section 2.7 (assignment2_submission_optimal_bias.npy).\n",
        "\n",
        "\n",
        "Note that untill section 2.9 you are not allowed to import additional packages **(especially not PyTorch)**. One of the objectives is to understand how the training procedure actually operates, before working with PyTorch's autograd engine which does it all for us. Importing the pytorch package will deduct from your points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47oq1vy5PUIV"
      },
      "source": [
        "## 1. Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "1aFWpuNSzGq9"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "def reload_functions():\n",
        "  if 'ML_DL_Functions2' in sys.modules:\n",
        "    del sys.modules['ML_DL_Functions2']\n",
        "  functions_path = drive_path.replace(\" \",\"\\ \") + 'ML_DL_Functions2.py'\n",
        "  !cp $functions_path ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7UWL6mFzGq-"
      },
      "source": [
        "Just like in the last assignment you should mount your google drive and make sure you have both the dataset from the moodle('YearPredictionMSD.csv') and the functions file ('ML_DL_Functions2.py') in the same directory which you will input below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY6PrfV4zGq_",
        "outputId": "0e195798-02da-4475-a3e6-c2505392f0cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive_path = '/content/gdrive/My Drive/ML/HW2/' # TODO - UPDATE ME WITH THE TRUE PATH!\n",
        "csv_path = drive_path + 'YearPredictionMSD.csv'\n",
        "t_label = [\"year\"]\n",
        "x_labels = [\"var%d\" % i for i in range(1, 91)]\n",
        "df = pandas.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgB83beNzGq_"
      },
      "source": [
        "Now that the data is loaded to your Colab notebook, you should be able to display the Pandas\n",
        "DataFrame `df` as a table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "H5bBEnj3zGq_",
        "outputId": "5bfd6bae-a833-45f6-9f4d-29b7b4791d41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        year      var1      var2      var3      var4      var5      var6  \\\n",
              "0       2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1       2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2       2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3       2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4       2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "...      ...       ...       ...       ...       ...       ...       ...   \n",
              "508710  1979  42.88385 -17.41629 -13.51726  -0.75243   4.74785  14.33437   \n",
              "508711  2010  42.47120  13.16539  -6.89795  14.78750  14.72776 -12.05820   \n",
              "508712  2010  45.21104 -19.03522 -16.50919  19.30722 -22.23290 -25.77296   \n",
              "508713  2004  44.60991  29.26510 -14.79970  16.26654 -20.44287  34.93228   \n",
              "508714  2002  51.58607  62.50479  10.33764  -7.60272 -35.88361 -30.27671   \n",
              "\n",
              "            var7      var8      var9  ...     var81      var82     var83  \\\n",
              "0      -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548  58.99367   \n",
              "1        8.76630  -0.92019  18.76548  ...   5.66812  -19.68073  33.04964   \n",
              "2       -3.27872  -2.35035  16.07017  ...   3.03800   26.05866 -50.92779   \n",
              "3        5.05097 -10.34124   3.55005  ...  34.57337 -171.70734 -16.96705   \n",
              "4      -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724  64.92712   \n",
              "...          ...       ...       ...  ...       ...        ...       ...   \n",
              "508710 -11.75670 -10.46058 -14.91937  ... -23.01045 -169.62524  43.90683   \n",
              "508711  -6.56437  -7.70141  -8.01135  ...   6.97510   97.98602 -45.39312   \n",
              "508712  15.66504  -3.26132   1.78980  ...  17.64373   27.46728  48.64159   \n",
              "508713  -8.15282   2.94035  -1.93460  ...  21.30827 -183.32526 -40.60815   \n",
              "508714   7.36746   0.33782   5.51725  ...   0.57308   34.69757 -22.81646   \n",
              "\n",
              "           var84     var85      var86      var87     var88      var89  \\\n",
              "0       15.37344   1.11144  -23.08793   68.40795  -1.82223  -27.46348   \n",
              "1       42.87836  -9.90378  -32.22788   70.49388  12.04941   58.43453   \n",
              "2       10.93792  -0.07568   43.20130 -115.00698  -0.05859   39.67068   \n",
              "3      -46.67617 -12.51516   82.58061  -72.08993   9.90558  199.62971   \n",
              "4      -17.72522  -1.49237   -7.50035   51.76631   7.88713   55.66926   \n",
              "...          ...       ...        ...        ...       ...        ...   \n",
              "508710  15.45299   2.84499   94.83469 -157.26665   3.60034   54.26775   \n",
              "508711 -30.26953  -9.49116  -51.58060  -12.08770   0.10696  117.82374   \n",
              "508712  92.03877  11.31597 -189.77886  179.06219  -3.74635  -27.01421   \n",
              "508713  19.53727  12.13429 -133.10456 -158.46478  22.36919  161.58392   \n",
              "508714  27.20861  -1.22381   -8.51102  -18.26440   2.22031  -93.94392   \n",
              "\n",
              "           var90  \n",
              "0        2.26327  \n",
              "1       26.92061  \n",
              "2       -0.66345  \n",
              "3       18.85382  \n",
              "4       28.74903  \n",
              "...          ...  \n",
              "508710 -22.24375  \n",
              "508711  -1.06577  \n",
              "508712 -10.23084  \n",
              "508713 -18.54131  \n",
              "508714  -6.75431  \n",
              "\n",
              "[508715 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-75f23e79-8fd1-4d22-a0ee-67ea97d57499\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508710</th>\n",
              "      <td>1979</td>\n",
              "      <td>42.88385</td>\n",
              "      <td>-17.41629</td>\n",
              "      <td>-13.51726</td>\n",
              "      <td>-0.75243</td>\n",
              "      <td>4.74785</td>\n",
              "      <td>14.33437</td>\n",
              "      <td>-11.75670</td>\n",
              "      <td>-10.46058</td>\n",
              "      <td>-14.91937</td>\n",
              "      <td>...</td>\n",
              "      <td>-23.01045</td>\n",
              "      <td>-169.62524</td>\n",
              "      <td>43.90683</td>\n",
              "      <td>15.45299</td>\n",
              "      <td>2.84499</td>\n",
              "      <td>94.83469</td>\n",
              "      <td>-157.26665</td>\n",
              "      <td>3.60034</td>\n",
              "      <td>54.26775</td>\n",
              "      <td>-22.24375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508711</th>\n",
              "      <td>2010</td>\n",
              "      <td>42.47120</td>\n",
              "      <td>13.16539</td>\n",
              "      <td>-6.89795</td>\n",
              "      <td>14.78750</td>\n",
              "      <td>14.72776</td>\n",
              "      <td>-12.05820</td>\n",
              "      <td>-6.56437</td>\n",
              "      <td>-7.70141</td>\n",
              "      <td>-8.01135</td>\n",
              "      <td>...</td>\n",
              "      <td>6.97510</td>\n",
              "      <td>97.98602</td>\n",
              "      <td>-45.39312</td>\n",
              "      <td>-30.26953</td>\n",
              "      <td>-9.49116</td>\n",
              "      <td>-51.58060</td>\n",
              "      <td>-12.08770</td>\n",
              "      <td>0.10696</td>\n",
              "      <td>117.82374</td>\n",
              "      <td>-1.06577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508712</th>\n",
              "      <td>2010</td>\n",
              "      <td>45.21104</td>\n",
              "      <td>-19.03522</td>\n",
              "      <td>-16.50919</td>\n",
              "      <td>19.30722</td>\n",
              "      <td>-22.23290</td>\n",
              "      <td>-25.77296</td>\n",
              "      <td>15.66504</td>\n",
              "      <td>-3.26132</td>\n",
              "      <td>1.78980</td>\n",
              "      <td>...</td>\n",
              "      <td>17.64373</td>\n",
              "      <td>27.46728</td>\n",
              "      <td>48.64159</td>\n",
              "      <td>92.03877</td>\n",
              "      <td>11.31597</td>\n",
              "      <td>-189.77886</td>\n",
              "      <td>179.06219</td>\n",
              "      <td>-3.74635</td>\n",
              "      <td>-27.01421</td>\n",
              "      <td>-10.23084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508713</th>\n",
              "      <td>2004</td>\n",
              "      <td>44.60991</td>\n",
              "      <td>29.26510</td>\n",
              "      <td>-14.79970</td>\n",
              "      <td>16.26654</td>\n",
              "      <td>-20.44287</td>\n",
              "      <td>34.93228</td>\n",
              "      <td>-8.15282</td>\n",
              "      <td>2.94035</td>\n",
              "      <td>-1.93460</td>\n",
              "      <td>...</td>\n",
              "      <td>21.30827</td>\n",
              "      <td>-183.32526</td>\n",
              "      <td>-40.60815</td>\n",
              "      <td>19.53727</td>\n",
              "      <td>12.13429</td>\n",
              "      <td>-133.10456</td>\n",
              "      <td>-158.46478</td>\n",
              "      <td>22.36919</td>\n",
              "      <td>161.58392</td>\n",
              "      <td>-18.54131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508714</th>\n",
              "      <td>2002</td>\n",
              "      <td>51.58607</td>\n",
              "      <td>62.50479</td>\n",
              "      <td>10.33764</td>\n",
              "      <td>-7.60272</td>\n",
              "      <td>-35.88361</td>\n",
              "      <td>-30.27671</td>\n",
              "      <td>7.36746</td>\n",
              "      <td>0.33782</td>\n",
              "      <td>5.51725</td>\n",
              "      <td>...</td>\n",
              "      <td>0.57308</td>\n",
              "      <td>34.69757</td>\n",
              "      <td>-22.81646</td>\n",
              "      <td>27.20861</td>\n",
              "      <td>-1.22381</td>\n",
              "      <td>-8.51102</td>\n",
              "      <td>-18.26440</td>\n",
              "      <td>2.22031</td>\n",
              "      <td>-93.94392</td>\n",
              "      <td>-6.75431</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>508715 rows Ã— 91 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75f23e79-8fd1-4d22-a0ee-67ea97d57499')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-75f23e79-8fd1-4d22-a0ee-67ea97d57499 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-75f23e79-8fd1-4d22-a0ee-67ea97d57499');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6f7047a7-5391-4dd7-9350-463f97f37fc1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6f7047a7-5391-4dd7-9350-463f97f37fc1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6f7047a7-5391-4dd7-9350-463f97f37fc1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_cda3c8e4-60b4-4ab4-97d0-d0a11980650f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_cda3c8e4-60b4-4ab4-97d0-d0a11980650f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaLuAMH_zGrA"
      },
      "source": [
        "To set up our data for classification, we'll use the \"year\" field to represent\n",
        "whether a song was released in the 20-th century. In our case `df[\"year\"]` will be 1 if\n",
        "the year was released after 2000, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "tZdGlNgdzGrA"
      },
      "outputs": [],
      "source": [
        "df[\"year\"] = df[\"year\"].map(lambda x: int(x > 2000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "xugy7FZ8eoAd",
        "outputId": "9d6a3fd2-147d-4da2-e23e-cdfc59e562b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    year      var1       var2      var3      var4      var5      var6  \\\n",
              "0      1  49.94357   21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1      1  48.73215   18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2      1  50.95714   31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3      1  48.24750   -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4      1  50.97020   42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "5      1  50.54767    0.31568  92.35066  22.38696 -25.51870 -19.04928   \n",
              "6      1  50.57546   33.17843  50.53517  11.55217 -27.24764  -8.78206   \n",
              "7      1  48.26892    8.97526  75.23158  24.04945 -16.02105 -14.09491   \n",
              "8      1  49.75468   33.99581  56.73846   2.89581  -2.92429 -26.44413   \n",
              "9      1  45.17809   46.34234 -40.65357  -2.47909   1.21253  -0.65302   \n",
              "10     1  39.13076  -23.01763 -36.20583   1.67519  -4.27101  13.01158   \n",
              "11     1  37.66498  -34.05910 -17.36060 -26.77781 -39.95119 -20.75000   \n",
              "12     1  26.51957 -148.15762 -13.30095  -7.25851  17.22029 -21.99439   \n",
              "13     1  37.68491  -26.84185 -27.10566 -14.95883  -5.87200 -21.68979   \n",
              "14     0  39.11695   -8.29767 -51.37966  -4.42668 -30.06506 -11.95916   \n",
              "15     1  35.05129  -67.97714 -14.20239  -6.68696  -0.61230 -18.70341   \n",
              "16     1  33.63129  -96.14912 -89.38216 -12.11699  13.77252  -6.69377   \n",
              "17     0  41.38639  -20.78665  51.80155  17.21415 -36.44189 -11.53169   \n",
              "18     0  37.45034   11.42615  56.28982  19.58426 -16.43530   2.22457   \n",
              "19     0  39.71092   -4.92800  12.88590 -11.87773   2.48031 -16.11028   \n",
              "\n",
              "        var7      var8      var9  ...     var81      var82      var83  \\\n",
              "0  -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n",
              "1    8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n",
              "2   -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n",
              "3    5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n",
              "4  -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n",
              "5   20.67345  -5.19943   3.63566  ...   6.59753  -50.69577   26.02574   \n",
              "6  -12.04282  -9.53930  28.61811  ...  11.63681   25.44182  134.62382   \n",
              "7    8.11871  -1.87566   7.46701  ...  18.03989  -58.46192  -65.56438   \n",
              "8    1.71392  -0.55644  22.08594  ...  18.70812    5.20391  -27.75192   \n",
              "9   -6.95536 -12.20040  17.02512  ...  -4.36742  -87.55285  -70.79677   \n",
              "10   8.05718  -8.41088   6.27370  ...  32.86051  -26.08461 -186.82429   \n",
              "11  -0.10231  -0.89972  -1.30205  ...  11.18909   45.20614   53.83925   \n",
              "12   5.51947   3.48418   2.61738  ...  23.80442  251.76360   18.81642   \n",
              "13   4.87374 -18.01800   1.52141  ... -67.57637  234.27192  -72.34557   \n",
              "14  -0.85322  -8.86179  11.36680  ...  42.22923  478.26580  -10.33823   \n",
              "15  -1.31928  -9.46370   5.53492  ...  10.25585   94.90539   15.95689   \n",
              "16 -33.36843 -24.81437  21.22757  ...  49.93249  -14.47489   40.70590   \n",
              "17  11.75252  -7.62428  -3.65488  ...  50.37614  -40.48205   48.07805   \n",
              "18   1.02668  -7.34736  -0.01184  ... -22.46207  -25.77228 -322.42841   \n",
              "19 -16.40421  -8.29657   9.86817  ...  11.92816  -73.72412   16.19039   \n",
              "\n",
              "        var84     var85      var86      var87     var88       var89     var90  \n",
              "0    15.37344   1.11144  -23.08793   68.40795  -1.82223   -27.46348   2.26327  \n",
              "1    42.87836  -9.90378  -32.22788   70.49388  12.04941    58.43453  26.92061  \n",
              "2    10.93792  -0.07568   43.20130 -115.00698  -0.05859    39.67068  -0.66345  \n",
              "3   -46.67617 -12.51516   82.58061  -72.08993   9.90558   199.62971  18.85382  \n",
              "4   -17.72522  -1.49237   -7.50035   51.76631   7.88713    55.66926  28.74903  \n",
              "5    18.94430  -0.33730    6.09352   35.18381   5.00283   -11.02257   0.02263  \n",
              "6    21.51982   8.17570   35.46251   11.57736   4.50056    -4.62739   1.40192  \n",
              "7    46.99856  -4.09602   56.37650  -18.29975  -0.30633     3.98364  -3.72556  \n",
              "8    17.22100  -0.85210  -15.67150  -26.36257   5.48708    -9.13495   6.08680  \n",
              "9    76.57355  -7.71727    3.26926 -298.49845  11.49326   -89.21804 -15.09719  \n",
              "10  113.58176   9.28727   44.60282  158.00425  -2.59543   109.19723  23.36143  \n",
              "11    2.59467  -4.00958  -47.74886 -170.92864  -5.19009     8.83617  -7.16056  \n",
              "12  157.09656 -27.79449 -137.72740  115.28414  23.00230  -164.02536  51.54138  \n",
              "13 -362.25101 -25.55019  -89.08971 -891.58937  14.11648 -1030.99180  99.28967  \n",
              "14 -103.76858  39.19511  -98.76636 -122.81061  -2.14942  -211.48202 -12.81569  \n",
              "15  -98.15732  -9.64859  -93.52834  -95.82981  20.73063  -562.07671  43.44696  \n",
              "16   58.63692   8.81522   27.28474    5.78046   3.44539   259.10825  10.28525  \n",
              "17   -7.62399   6.51934  -30.46090  -53.87264   4.44627    58.16913  -0.02409  \n",
              "18 -146.57408  13.61588   92.22918 -439.80259  25.73235   157.22967  38.70617  \n",
              "19    9.79606   9.71693   -9.90907  -20.65851   2.34002   -31.57015   1.58400  \n",
              "\n",
              "[20 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c08317f-4c46-4867-8df9-35ef49e5edeb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>50.54767</td>\n",
              "      <td>0.31568</td>\n",
              "      <td>92.35066</td>\n",
              "      <td>22.38696</td>\n",
              "      <td>-25.51870</td>\n",
              "      <td>-19.04928</td>\n",
              "      <td>20.67345</td>\n",
              "      <td>-5.19943</td>\n",
              "      <td>3.63566</td>\n",
              "      <td>...</td>\n",
              "      <td>6.59753</td>\n",
              "      <td>-50.69577</td>\n",
              "      <td>26.02574</td>\n",
              "      <td>18.94430</td>\n",
              "      <td>-0.33730</td>\n",
              "      <td>6.09352</td>\n",
              "      <td>35.18381</td>\n",
              "      <td>5.00283</td>\n",
              "      <td>-11.02257</td>\n",
              "      <td>0.02263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>50.57546</td>\n",
              "      <td>33.17843</td>\n",
              "      <td>50.53517</td>\n",
              "      <td>11.55217</td>\n",
              "      <td>-27.24764</td>\n",
              "      <td>-8.78206</td>\n",
              "      <td>-12.04282</td>\n",
              "      <td>-9.53930</td>\n",
              "      <td>28.61811</td>\n",
              "      <td>...</td>\n",
              "      <td>11.63681</td>\n",
              "      <td>25.44182</td>\n",
              "      <td>134.62382</td>\n",
              "      <td>21.51982</td>\n",
              "      <td>8.17570</td>\n",
              "      <td>35.46251</td>\n",
              "      <td>11.57736</td>\n",
              "      <td>4.50056</td>\n",
              "      <td>-4.62739</td>\n",
              "      <td>1.40192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>48.26892</td>\n",
              "      <td>8.97526</td>\n",
              "      <td>75.23158</td>\n",
              "      <td>24.04945</td>\n",
              "      <td>-16.02105</td>\n",
              "      <td>-14.09491</td>\n",
              "      <td>8.11871</td>\n",
              "      <td>-1.87566</td>\n",
              "      <td>7.46701</td>\n",
              "      <td>...</td>\n",
              "      <td>18.03989</td>\n",
              "      <td>-58.46192</td>\n",
              "      <td>-65.56438</td>\n",
              "      <td>46.99856</td>\n",
              "      <td>-4.09602</td>\n",
              "      <td>56.37650</td>\n",
              "      <td>-18.29975</td>\n",
              "      <td>-0.30633</td>\n",
              "      <td>3.98364</td>\n",
              "      <td>-3.72556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>49.75468</td>\n",
              "      <td>33.99581</td>\n",
              "      <td>56.73846</td>\n",
              "      <td>2.89581</td>\n",
              "      <td>-2.92429</td>\n",
              "      <td>-26.44413</td>\n",
              "      <td>1.71392</td>\n",
              "      <td>-0.55644</td>\n",
              "      <td>22.08594</td>\n",
              "      <td>...</td>\n",
              "      <td>18.70812</td>\n",
              "      <td>5.20391</td>\n",
              "      <td>-27.75192</td>\n",
              "      <td>17.22100</td>\n",
              "      <td>-0.85210</td>\n",
              "      <td>-15.67150</td>\n",
              "      <td>-26.36257</td>\n",
              "      <td>5.48708</td>\n",
              "      <td>-9.13495</td>\n",
              "      <td>6.08680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>45.17809</td>\n",
              "      <td>46.34234</td>\n",
              "      <td>-40.65357</td>\n",
              "      <td>-2.47909</td>\n",
              "      <td>1.21253</td>\n",
              "      <td>-0.65302</td>\n",
              "      <td>-6.95536</td>\n",
              "      <td>-12.20040</td>\n",
              "      <td>17.02512</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.36742</td>\n",
              "      <td>-87.55285</td>\n",
              "      <td>-70.79677</td>\n",
              "      <td>76.57355</td>\n",
              "      <td>-7.71727</td>\n",
              "      <td>3.26926</td>\n",
              "      <td>-298.49845</td>\n",
              "      <td>11.49326</td>\n",
              "      <td>-89.21804</td>\n",
              "      <td>-15.09719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>39.13076</td>\n",
              "      <td>-23.01763</td>\n",
              "      <td>-36.20583</td>\n",
              "      <td>1.67519</td>\n",
              "      <td>-4.27101</td>\n",
              "      <td>13.01158</td>\n",
              "      <td>8.05718</td>\n",
              "      <td>-8.41088</td>\n",
              "      <td>6.27370</td>\n",
              "      <td>...</td>\n",
              "      <td>32.86051</td>\n",
              "      <td>-26.08461</td>\n",
              "      <td>-186.82429</td>\n",
              "      <td>113.58176</td>\n",
              "      <td>9.28727</td>\n",
              "      <td>44.60282</td>\n",
              "      <td>158.00425</td>\n",
              "      <td>-2.59543</td>\n",
              "      <td>109.19723</td>\n",
              "      <td>23.36143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>37.66498</td>\n",
              "      <td>-34.05910</td>\n",
              "      <td>-17.36060</td>\n",
              "      <td>-26.77781</td>\n",
              "      <td>-39.95119</td>\n",
              "      <td>-20.75000</td>\n",
              "      <td>-0.10231</td>\n",
              "      <td>-0.89972</td>\n",
              "      <td>-1.30205</td>\n",
              "      <td>...</td>\n",
              "      <td>11.18909</td>\n",
              "      <td>45.20614</td>\n",
              "      <td>53.83925</td>\n",
              "      <td>2.59467</td>\n",
              "      <td>-4.00958</td>\n",
              "      <td>-47.74886</td>\n",
              "      <td>-170.92864</td>\n",
              "      <td>-5.19009</td>\n",
              "      <td>8.83617</td>\n",
              "      <td>-7.16056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>26.51957</td>\n",
              "      <td>-148.15762</td>\n",
              "      <td>-13.30095</td>\n",
              "      <td>-7.25851</td>\n",
              "      <td>17.22029</td>\n",
              "      <td>-21.99439</td>\n",
              "      <td>5.51947</td>\n",
              "      <td>3.48418</td>\n",
              "      <td>2.61738</td>\n",
              "      <td>...</td>\n",
              "      <td>23.80442</td>\n",
              "      <td>251.76360</td>\n",
              "      <td>18.81642</td>\n",
              "      <td>157.09656</td>\n",
              "      <td>-27.79449</td>\n",
              "      <td>-137.72740</td>\n",
              "      <td>115.28414</td>\n",
              "      <td>23.00230</td>\n",
              "      <td>-164.02536</td>\n",
              "      <td>51.54138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>37.68491</td>\n",
              "      <td>-26.84185</td>\n",
              "      <td>-27.10566</td>\n",
              "      <td>-14.95883</td>\n",
              "      <td>-5.87200</td>\n",
              "      <td>-21.68979</td>\n",
              "      <td>4.87374</td>\n",
              "      <td>-18.01800</td>\n",
              "      <td>1.52141</td>\n",
              "      <td>...</td>\n",
              "      <td>-67.57637</td>\n",
              "      <td>234.27192</td>\n",
              "      <td>-72.34557</td>\n",
              "      <td>-362.25101</td>\n",
              "      <td>-25.55019</td>\n",
              "      <td>-89.08971</td>\n",
              "      <td>-891.58937</td>\n",
              "      <td>14.11648</td>\n",
              "      <td>-1030.99180</td>\n",
              "      <td>99.28967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>39.11695</td>\n",
              "      <td>-8.29767</td>\n",
              "      <td>-51.37966</td>\n",
              "      <td>-4.42668</td>\n",
              "      <td>-30.06506</td>\n",
              "      <td>-11.95916</td>\n",
              "      <td>-0.85322</td>\n",
              "      <td>-8.86179</td>\n",
              "      <td>11.36680</td>\n",
              "      <td>...</td>\n",
              "      <td>42.22923</td>\n",
              "      <td>478.26580</td>\n",
              "      <td>-10.33823</td>\n",
              "      <td>-103.76858</td>\n",
              "      <td>39.19511</td>\n",
              "      <td>-98.76636</td>\n",
              "      <td>-122.81061</td>\n",
              "      <td>-2.14942</td>\n",
              "      <td>-211.48202</td>\n",
              "      <td>-12.81569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>35.05129</td>\n",
              "      <td>-67.97714</td>\n",
              "      <td>-14.20239</td>\n",
              "      <td>-6.68696</td>\n",
              "      <td>-0.61230</td>\n",
              "      <td>-18.70341</td>\n",
              "      <td>-1.31928</td>\n",
              "      <td>-9.46370</td>\n",
              "      <td>5.53492</td>\n",
              "      <td>...</td>\n",
              "      <td>10.25585</td>\n",
              "      <td>94.90539</td>\n",
              "      <td>15.95689</td>\n",
              "      <td>-98.15732</td>\n",
              "      <td>-9.64859</td>\n",
              "      <td>-93.52834</td>\n",
              "      <td>-95.82981</td>\n",
              "      <td>20.73063</td>\n",
              "      <td>-562.07671</td>\n",
              "      <td>43.44696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>33.63129</td>\n",
              "      <td>-96.14912</td>\n",
              "      <td>-89.38216</td>\n",
              "      <td>-12.11699</td>\n",
              "      <td>13.77252</td>\n",
              "      <td>-6.69377</td>\n",
              "      <td>-33.36843</td>\n",
              "      <td>-24.81437</td>\n",
              "      <td>21.22757</td>\n",
              "      <td>...</td>\n",
              "      <td>49.93249</td>\n",
              "      <td>-14.47489</td>\n",
              "      <td>40.70590</td>\n",
              "      <td>58.63692</td>\n",
              "      <td>8.81522</td>\n",
              "      <td>27.28474</td>\n",
              "      <td>5.78046</td>\n",
              "      <td>3.44539</td>\n",
              "      <td>259.10825</td>\n",
              "      <td>10.28525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>41.38639</td>\n",
              "      <td>-20.78665</td>\n",
              "      <td>51.80155</td>\n",
              "      <td>17.21415</td>\n",
              "      <td>-36.44189</td>\n",
              "      <td>-11.53169</td>\n",
              "      <td>11.75252</td>\n",
              "      <td>-7.62428</td>\n",
              "      <td>-3.65488</td>\n",
              "      <td>...</td>\n",
              "      <td>50.37614</td>\n",
              "      <td>-40.48205</td>\n",
              "      <td>48.07805</td>\n",
              "      <td>-7.62399</td>\n",
              "      <td>6.51934</td>\n",
              "      <td>-30.46090</td>\n",
              "      <td>-53.87264</td>\n",
              "      <td>4.44627</td>\n",
              "      <td>58.16913</td>\n",
              "      <td>-0.02409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>37.45034</td>\n",
              "      <td>11.42615</td>\n",
              "      <td>56.28982</td>\n",
              "      <td>19.58426</td>\n",
              "      <td>-16.43530</td>\n",
              "      <td>2.22457</td>\n",
              "      <td>1.02668</td>\n",
              "      <td>-7.34736</td>\n",
              "      <td>-0.01184</td>\n",
              "      <td>...</td>\n",
              "      <td>-22.46207</td>\n",
              "      <td>-25.77228</td>\n",
              "      <td>-322.42841</td>\n",
              "      <td>-146.57408</td>\n",
              "      <td>13.61588</td>\n",
              "      <td>92.22918</td>\n",
              "      <td>-439.80259</td>\n",
              "      <td>25.73235</td>\n",
              "      <td>157.22967</td>\n",
              "      <td>38.70617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>39.71092</td>\n",
              "      <td>-4.92800</td>\n",
              "      <td>12.88590</td>\n",
              "      <td>-11.87773</td>\n",
              "      <td>2.48031</td>\n",
              "      <td>-16.11028</td>\n",
              "      <td>-16.40421</td>\n",
              "      <td>-8.29657</td>\n",
              "      <td>9.86817</td>\n",
              "      <td>...</td>\n",
              "      <td>11.92816</td>\n",
              "      <td>-73.72412</td>\n",
              "      <td>16.19039</td>\n",
              "      <td>9.79606</td>\n",
              "      <td>9.71693</td>\n",
              "      <td>-9.90907</td>\n",
              "      <td>-20.65851</td>\n",
              "      <td>2.34002</td>\n",
              "      <td>-31.57015</td>\n",
              "      <td>1.58400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows Ã— 91 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c08317f-4c46-4867-8df9-35ef49e5edeb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5c08317f-4c46-4867-8df9-35ef49e5edeb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5c08317f-4c46-4867-8df9-35ef49e5edeb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9f42a1fd-d8f5-4682-aabf-c6b865e7a3e5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9f42a1fd-d8f5-4682-aabf-c6b865e7a3e5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9f42a1fd-d8f5-4682-aabf-c6b865e7a3e5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "df.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncjxI4WdzGrA"
      },
      "source": [
        "### 1.1 - Train Test Split\n",
        "\n",
        "The data set description text asks us to respect the below train/test split to\n",
        "avoid the \"producer effect\". That is, we want to make sure that no song from a single artist\n",
        "ends up in both the training and test set.\n",
        "\n",
        "#### Food for thought:\n",
        "why would it be problematic to have some songs from an artist in the training set, and other songs from the same artist in the test set. (Hint: Remember that we want our test accuracy to predict how well the model will perform in practice on a song it hasn't learned about.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "2NiYlxpFzGrB"
      },
      "outputs": [],
      "source": [
        "# train test split\n",
        "df_train = df[:463715]\n",
        "df_test = df[463715:]\n",
        "\n",
        "# convert to numpy\n",
        "train_xs = df_train[x_labels].to_numpy()\n",
        "train_ts = df_train[t_label].to_numpy()\n",
        "test_xs = df_test[x_labels].to_numpy()\n",
        "test_ts = df_test[t_label].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYSzd4XUzGrB"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "Normalize the data by subtracting the mean and dividing by the std just like the last assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "TPuWLksJzGrB"
      },
      "outputs": [],
      "source": [
        "feature_means = train_xs.mean(axis=0)\n",
        "feature_stds  = train_xs.std(axis=0)\n",
        "#feature_stds[feature_stds==0] = 0.01\n",
        "train_norm_xs = (train_xs - feature_means) / feature_stds\n",
        "test_norm_xs = (test_xs - feature_means) / feature_stds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4GqL5J_zGrC"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "Finally, we'll move some of the data in our training set into a validation set.\n",
        "#### Food for thought:\n",
        "Why should we limit how many times we use the test set, and how do we use the validation set during the model building process?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "HsXv1U3gzGrC"
      },
      "outputs": [],
      "source": [
        "# shuffle the training set\n",
        "reindex = np.random.permutation(len(train_xs))\n",
        "train_xs = train_xs[reindex]\n",
        "train_norm_xs = train_norm_xs[reindex]\n",
        "train_ts = train_ts[reindex]\n",
        "\n",
        "# use the first 50000 elements of `train_xs` as the validation set\n",
        "train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n",
        "train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n",
        "train_ts, val_ts           = train_ts[50000:], train_ts[:50000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy4lt445zGrD"
      },
      "source": [
        "## Part 2. Classification (79%)\n",
        "\n",
        "We will first build a *classification* model to perform decade classification. We have written a few helper functions for you. You can find them in your functions file ('sigmoid', 'cross_entropy' and 'get_accuracy'). All other code that you write in this section should be vectorized whenever possible (i.e., avoid unnecessary loops). Feel free to add more testing to the notebook to validate your code in the functions file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ZIfooBzGrD"
      },
      "source": [
        "### 2.1 Prediction\n",
        "\n",
        "Fill in the function `pred` in the functions file that computes the prediction `y` based on logistic regression, i.e., a single layer with weights `w` and bias `b`. The output is given by:\n",
        "\\begin{equation}\n",
        "y = \\sigma({\\bf w}^T {\\bf x} + b),\n",
        "\\end{equation}\n",
        "where the value of $y$ is an estimate of the probability that the song is released in the current century, namely ${\\rm year} =1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naY5mT4_zGrD",
        "outputId": "271b35ad-dc4b-43f1-e0c2-a1c2692491d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.73105858, 0.73105858])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "ML_DL_Functions2.pred(np.zeros(90), 1, np.ones([2, 90]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCrT4b0jlxhI"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fOu-ekEJ0dn"
      },
      "source": [
        "### 2.2 Cost\n",
        "Assuming the loss function is the cross entropy function fill in the cost(risk) function in the functions file which returns the mean of the loss function on all inputs.\n",
        "$$\\mathcal{L}_\\mathcal{P}(\\text{Cross Entropy}) = \\mathbb{E}_{(y,t)\\sim\\mathcal{P}}\\left\\{\\text{CE}(t,s)\\right\\}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmWuoJocXFe4",
        "outputId": "95e65b94-7258-4a32-8a71-3ac8a9a01072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6931471805599453\n"
          ]
        }
      ],
      "source": [
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "print(ML_DL_Functions2.cost(0.5*np.ones(4), np.ones(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxNdmSd3zGrE"
      },
      "source": [
        "### 2.3 Derivative of the cost -- 7%\n",
        "Take a pen and paper and calculate the analytical derivative of the cost function with respect to the weights and bias. use the formula calculated to fill in the function `derivative_cost` that computes and returns the gradients\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$ and\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial b}$. Here, `X` is the input, `y` is the prediction, and `t` is the true label.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P80bu7qmzGrE",
        "outputId": "8c770ded-4f11-44f2-f0ab-6cfdf4062462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90,)\n",
            "<class 'numpy.float64'>\n"
          ]
        }
      ],
      "source": [
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "dldw, dldb = ML_DL_Functions2.derivative_cost(np.ones([10,90]), np.ones(10), np.ones(10))\n",
        "print(dldw.shape)\n",
        "print(type(dldb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhQXAKd4zGrE"
      },
      "source": [
        "### 2.4 Derivative approximation\n",
        "\n",
        "We can check that our derivative is implemented correctly using the finite difference rule. In 1D, the\n",
        "finite difference rule tells us that for small $h$, we should have\n",
        "\n",
        "$$\\frac{f(x+h) - f(x)}{h} \\approx f'(x)$$\n",
        "\n",
        "make sure that $\\frac{\\partial\\mathcal{L}}{\\partial b}$  is implement correctly\n",
        "by comparing the result from `derivative_cost` with the empirical cost derivative computed using the above numerical approximation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpRTD-fozGrF",
        "outputId": "9f467740-4017-4a0e-fae8-9726d31e278f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analytical results is - -0.3775406687981454\n",
            "The algorithm results is -  -0.3775394937899356\n"
          ]
        }
      ],
      "source": [
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "# Your code goes here\n",
        "N, d = 10, 90\n",
        "X = np.ones((N, d))\n",
        "t = np.ones(N)\n",
        "b = 0.5\n",
        "y = 1 / (1 + np.exp(-(X @ np.zeros(d) + b)))\n",
        "w = np.zeros(90)\n",
        "dldw, r1 = ML_DL_Functions2.derivative_cost(X, y, t)\n",
        "\n",
        "h = 1e-5\n",
        "L1 = ML_DL_Functions2.cost(ML_DL_Functions2.pred(w,b,X), t)\n",
        "L2 = ML_DL_Functions2.cost(ML_DL_Functions2.pred(w,b+h,X), t)\n",
        "r2 = (L2 - L1) / h\n",
        "\n",
        "print(\"The analytical results is -\", r1)\n",
        "print(\"The algorithm results is - \", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTiplTPhzGrF"
      },
      "source": [
        "make sure that $\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$  is implement correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVTsHgnPzGrF",
        "outputId": "edb51ad8-f439-4fc7-87b4-6263ec1ae721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analytical results is - [-0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067\n",
            " -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067 -0.37754067]\n",
            "The algorithm results is -  [-0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949\n",
            " -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949 -0.37753949]\n"
          ]
        }
      ],
      "source": [
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "\n",
        "N, d = 10, 90\n",
        "X = np.ones((N, d))\n",
        "t = np.ones(N)\n",
        "b = 0.5\n",
        "y = 1 / (1 + np.exp(-(X @ np.zeros(d) + b)))\n",
        "w = np.zeros(90)\n",
        "r1, dldb = ML_DL_Functions2.derivative_cost(X, y, t)\n",
        "\n",
        "h = 1e-5\n",
        "r2 = np.zeros_like(w)\n",
        "\n",
        "for j in range(d):\n",
        "    w_perturbed = w.copy()\n",
        "    w_perturbed[j] += h\n",
        "    L1 = ML_DL_Functions2.cost(ML_DL_Functions2.pred(w, b, X), t)\n",
        "    L2 = ML_DL_Functions2.cost(ML_DL_Functions2.pred(w_perturbed, b, X), t)\n",
        "    r2[j] = (L2 - L1) / h\n",
        "\n",
        "print(\"The analytical results is -\", r1)\n",
        "print(\"The algorithm results is - \", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgBTPF_2zGrG"
      },
      "source": [
        "### 2.5 Gradient descent\n",
        "\n",
        "Now that you have a gradient function that works, we can actually run gradient descent.\n",
        "Complete the following code that will run stochastic: gradient descent training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "nW4DEuuPzGrG"
      },
      "outputs": [],
      "source": [
        "def run_gradient_descent(w0, b0, val_norm_xs, val_ts, train_norm_xs, train_ts, mu=0.1, batch_size=100, max_iters=100):\n",
        "  \"\"\"Return the values of (w, b) after running gradient descent for max_iters.\n",
        "  We use:\n",
        "    - train_norm_xs and train_ts as the training set\n",
        "    - val_norm_xs and val_ts as the test set\n",
        "    - mu as the learning rate\n",
        "    - (w0, b0) as the initial values of (w, b)\n",
        "\n",
        "  Precondition: np.shape(w0) == (90,)\n",
        "                type(b0) == float\n",
        "\n",
        "  Postcondition: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "  \"\"\"\n",
        "\n",
        "  w = w0\n",
        "  b = b0\n",
        "  iter = -1 if max_iters % 40 != 0 else 0   #this helps me overcome an error\n",
        "  max_acc = 0\n",
        "  opt_w = w\n",
        "  opt_b = b\n",
        "  cost_list = []\n",
        "  acc_list  = []\n",
        "  while iter < max_iters:\n",
        "    # shuffle the training set (there is code above for how to do this)\n",
        "    # <===\n",
        "    # shuffle the training set\n",
        "\n",
        "    reindex = np.random.permutation(len(train_norm_xs))\n",
        "    train_norm_xs = train_norm_xs[reindex]\n",
        "    train_ts = train_ts[reindex]\n",
        "\n",
        "    for i in range(0, len(train_norm_xs), batch_size): # iterate over each minibatch\n",
        "      # minibatch that we are working with:\n",
        "      X = train_norm_xs[i:(i + batch_size)]\n",
        "      t = train_ts[i:(i + batch_size), 0]\n",
        "\n",
        "      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n",
        "      # the \"last\" minibatch\n",
        "      if np.shape(X)[0] != batch_size:\n",
        "        continue\n",
        "\n",
        "      # compute the prediction\n",
        "      # <===\n",
        "      y = ML_DL_Functions2.pred(w, b, X)\n",
        "      # calculate gradient(backpropegate)\n",
        "      # <===\n",
        "      dldw, dldb = ML_DL_Functions2.derivative_cost(X, y, t)\n",
        "      # update w and b(step)\n",
        "      # <===\n",
        "      w = w - mu * dldw\n",
        "      b = b - mu * dldb\n",
        "      # increment the iteration count\n",
        "      iter += 1\n",
        "      # compute and print the *validation* loss and accuracy\n",
        "      if iter % 40 == 0:\n",
        "        # <===\n",
        "        # if j + val_batch_size >= len(val_norm_xs):\n",
        "        #   j = 0\n",
        "        # X_val_batch = val_norm_xs[j:(j + val_batch_size)]\n",
        "        # t_val_batch = val_ts[j:(j + val_batch_size), 0]\n",
        "        # j += val_batch_size\n",
        "\n",
        "        val_preds = ML_DL_Functions2.pred(w, b, val_norm_xs)\n",
        "        val_cost = ML_DL_Functions2.cost(val_preds, val_ts[:, 0])\n",
        "        val_acc = ML_DL_Functions2.get_accuracy(val_preds, val_ts)\n",
        "        cost_list.append(val_cost)\n",
        "        acc_list.append(val_acc)\n",
        "        # save the best weights and biases\n",
        "        if val_acc>max_acc:\n",
        "          opt_w = w\n",
        "          opt_b = b\n",
        "          max_acc = val_acc\n",
        "\n",
        "        print(\"Iter %d. [Val Acc %.0f%%, Loss %f]\" % (\n",
        "              iter, val_acc * 100, val_cost))\n",
        "\n",
        "      if iter >= max_iters:\n",
        "        break\n",
        "\n",
        "\n",
        "  return opt_w, opt_b, cost_list, acc_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MqzT0jGzGrH"
      },
      "source": [
        "### 2.6 Running everything!\n",
        "\n",
        "Call `run_gradient_descent` with the weights and biases all initialized to zero. Test your self with different $\\mu$ values and show that if mu is too small then convergance is slow and if mu is too large then the optimization algorithm does not converge. You can add more automation and plot function to help you find the best configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tE32Iqo6zGrH",
        "outputId": "2160b5b4-1cf1-413d-a1da-b9d575ad1621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 40. [Val Acc 67%, Loss 0.656674]\n",
            "Iter 80. [Val Acc 69%, Loss 0.636964]\n",
            "Iter 120. [Val Acc 70%, Loss 0.625098]\n",
            "Iter 160. [Val Acc 70%, Loss 0.617206]\n",
            "Iter 200. [Val Acc 70%, Loss 0.610624]\n",
            "Iter 240. [Val Acc 71%, Loss 0.605746]\n",
            "Iter 280. [Val Acc 71%, Loss 0.601442]\n",
            "Iter 320. [Val Acc 71%, Loss 0.597888]\n",
            "Iter 360. [Val Acc 71%, Loss 0.594263]\n",
            "Iter 400. [Val Acc 71%, Loss 0.591650]\n",
            "Iter 440. [Val Acc 71%, Loss 0.589067]\n",
            "Iter 480. [Val Acc 72%, Loss 0.586848]\n",
            "Iter 520. [Val Acc 72%, Loss 0.584950]\n",
            "Iter 560. [Val Acc 72%, Loss 0.583312]\n",
            "Iter 600. [Val Acc 72%, Loss 0.582094]\n",
            "Iter 640. [Val Acc 72%, Loss 0.580940]\n",
            "Iter 680. [Val Acc 72%, Loss 0.579238]\n",
            "Iter 720. [Val Acc 72%, Loss 0.578025]\n",
            "Iter 760. [Val Acc 72%, Loss 0.577143]\n",
            "Iter 800. [Val Acc 72%, Loss 0.576351]\n",
            "Iter 840. [Val Acc 72%, Loss 0.575424]\n",
            "Iter 880. [Val Acc 72%, Loss 0.574705]\n",
            "Iter 920. [Val Acc 72%, Loss 0.574442]\n",
            "Iter 960. [Val Acc 72%, Loss 0.573724]\n",
            "Iter 1000. [Val Acc 72%, Loss 0.573118]\n",
            "Iter 1040. [Val Acc 73%, Loss 0.572220]\n",
            "Iter 1080. [Val Acc 73%, Loss 0.571728]\n",
            "Iter 1120. [Val Acc 73%, Loss 0.571177]\n",
            "Iter 1160. [Val Acc 73%, Loss 0.570428]\n",
            "Iter 1200. [Val Acc 73%, Loss 0.570815]\n",
            "Iter 1240. [Val Acc 73%, Loss 0.570658]\n",
            "Iter 1280. [Val Acc 73%, Loss 0.569937]\n",
            "Iter 1320. [Val Acc 73%, Loss 0.569721]\n",
            "Iter 1360. [Val Acc 73%, Loss 0.568888]\n",
            "Iter 1400. [Val Acc 73%, Loss 0.568624]\n",
            "Iter 1440. [Val Acc 73%, Loss 0.567803]\n",
            "Iter 1480. [Val Acc 73%, Loss 0.567420]\n",
            "Iter 1520. [Val Acc 73%, Loss 0.567366]\n",
            "Iter 1560. [Val Acc 73%, Loss 0.567437]\n",
            "Iter 1600. [Val Acc 73%, Loss 0.567052]\n",
            "Iter 1640. [Val Acc 73%, Loss 0.566754]\n",
            "Iter 1680. [Val Acc 73%, Loss 0.566752]\n",
            "Iter 1720. [Val Acc 73%, Loss 0.566610]\n",
            "Iter 1760. [Val Acc 73%, Loss 0.566307]\n",
            "Iter 1800. [Val Acc 73%, Loss 0.566922]\n",
            "Iter 1840. [Val Acc 73%, Loss 0.566127]\n",
            "Iter 1880. [Val Acc 73%, Loss 0.566011]\n",
            "Iter 1920. [Val Acc 73%, Loss 0.565745]\n",
            "Iter 1960. [Val Acc 73%, Loss 0.565587]\n",
            "Iter 2000. [Val Acc 73%, Loss 0.565314]\n",
            "Iter 2040. [Val Acc 73%, Loss 0.565944]\n",
            "Iter 2080. [Val Acc 73%, Loss 0.565016]\n",
            "Iter 2120. [Val Acc 73%, Loss 0.565042]\n",
            "Iter 2160. [Val Acc 73%, Loss 0.564811]\n",
            "Iter 2200. [Val Acc 73%, Loss 0.564489]\n",
            "Iter 2240. [Val Acc 73%, Loss 0.564441]\n",
            "Iter 2280. [Val Acc 73%, Loss 0.564858]\n",
            "Iter 2320. [Val Acc 73%, Loss 0.564591]\n",
            "Iter 2360. [Val Acc 73%, Loss 0.564697]\n",
            "Iter 2400. [Val Acc 73%, Loss 0.564649]\n",
            "Iter 2440. [Val Acc 73%, Loss 0.564258]\n",
            "Iter 2480. [Val Acc 73%, Loss 0.563880]\n",
            "Iter 2520. [Val Acc 73%, Loss 0.563876]\n",
            "Iter 2560. [Val Acc 73%, Loss 0.564047]\n",
            "Iter 2600. [Val Acc 73%, Loss 0.563819]\n",
            "Iter 2640. [Val Acc 73%, Loss 0.564371]\n",
            "Iter 2680. [Val Acc 73%, Loss 0.564010]\n",
            "Iter 2720. [Val Acc 73%, Loss 0.563591]\n",
            "Iter 2760. [Val Acc 73%, Loss 0.563628]\n",
            "Iter 2800. [Val Acc 73%, Loss 0.563775]\n",
            "Iter 2840. [Val Acc 73%, Loss 0.564149]\n",
            "Iter 2880. [Val Acc 73%, Loss 0.563676]\n",
            "Iter 2920. [Val Acc 73%, Loss 0.564120]\n",
            "Iter 2960. [Val Acc 73%, Loss 0.564247]\n",
            "Iter 3000. [Val Acc 73%, Loss 0.564008]\n",
            "Iter 3040. [Val Acc 73%, Loss 0.564269]\n",
            "Iter 3080. [Val Acc 73%, Loss 0.564682]\n",
            "Iter 3120. [Val Acc 73%, Loss 0.564085]\n",
            "Iter 3160. [Val Acc 73%, Loss 0.563866]\n",
            "Iter 3200. [Val Acc 73%, Loss 0.564430]\n",
            "Iter 3240. [Val Acc 73%, Loss 0.563393]\n",
            "Iter 3280. [Val Acc 73%, Loss 0.563591]\n",
            "Iter 3320. [Val Acc 73%, Loss 0.563391]\n",
            "Iter 3360. [Val Acc 73%, Loss 0.563389]\n",
            "Iter 3400. [Val Acc 73%, Loss 0.563666]\n",
            "Iter 3440. [Val Acc 73%, Loss 0.563732]\n",
            "Iter 3480. [Val Acc 73%, Loss 0.563463]\n",
            "Iter 3520. [Val Acc 73%, Loss 0.563457]\n",
            "Iter 3560. [Val Acc 73%, Loss 0.563718]\n",
            "Iter 3600. [Val Acc 73%, Loss 0.563396]\n",
            "Iter 3640. [Val Acc 73%, Loss 0.563315]\n",
            "Iter 3680. [Val Acc 73%, Loss 0.563431]\n",
            "Iter 3720. [Val Acc 73%, Loss 0.563373]\n",
            "Iter 3760. [Val Acc 73%, Loss 0.563117]\n",
            "Iter 3800. [Val Acc 73%, Loss 0.563175]\n",
            "Iter 3840. [Val Acc 73%, Loss 0.563115]\n",
            "Iter 3880. [Val Acc 73%, Loss 0.563236]\n",
            "Iter 3920. [Val Acc 73%, Loss 0.563396]\n",
            "Iter 3960. [Val Acc 73%, Loss 0.563311]\n",
            "Iter 4000. [Val Acc 73%, Loss 0.563152]\n",
            "Iter 4040. [Val Acc 73%, Loss 0.563519]\n",
            "Iter 4080. [Val Acc 73%, Loss 0.563207]\n",
            "Iter 4120. [Val Acc 73%, Loss 0.563061]\n",
            "Iter 4160. [Val Acc 73%, Loss 0.563362]\n",
            "Iter 4200. [Val Acc 73%, Loss 0.563238]\n",
            "Iter 4240. [Val Acc 73%, Loss 0.562711]\n",
            "Iter 4280. [Val Acc 73%, Loss 0.562583]\n",
            "Iter 4320. [Val Acc 73%, Loss 0.563101]\n",
            "Iter 4360. [Val Acc 73%, Loss 0.562636]\n",
            "Iter 4400. [Val Acc 73%, Loss 0.562445]\n",
            "Iter 4440. [Val Acc 73%, Loss 0.562861]\n",
            "Iter 4480. [Val Acc 73%, Loss 0.563027]\n",
            "Iter 4520. [Val Acc 73%, Loss 0.563058]\n",
            "Iter 4560. [Val Acc 73%, Loss 0.563193]\n",
            "Iter 4600. [Val Acc 73%, Loss 0.563384]\n",
            "Iter 4640. [Val Acc 73%, Loss 0.562828]\n",
            "Iter 4680. [Val Acc 73%, Loss 0.563123]\n",
            "Iter 4720. [Val Acc 73%, Loss 0.562939]\n",
            "Iter 4760. [Val Acc 73%, Loss 0.562932]\n",
            "Iter 4800. [Val Acc 73%, Loss 0.563217]\n",
            "Iter 4840. [Val Acc 73%, Loss 0.563160]\n",
            "Iter 4880. [Val Acc 73%, Loss 0.562863]\n",
            "Iter 4920. [Val Acc 73%, Loss 0.562729]\n",
            "Iter 4960. [Val Acc 73%, Loss 0.562903]\n",
            "Iter 5000. [Val Acc 73%, Loss 0.563000]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'classification accuracy per iteration; $\\\\mu$=0.02 batch size=100')"
            ]
          },
          "metadata": {},
          "execution_count": 126
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "w0 = np.zeros(90)\n",
        "b0 = np.zeros(1)[0]\n",
        "\n",
        "# choose values\n",
        "mu = 0.02\n",
        "max_iters = 5000\n",
        "batch_size = 100\n",
        "\n",
        "# Write your code here\n",
        "\n",
        "opt_w,opt_b,cost_list,acc_list = run_gradient_descent(w0,b0, val_norm_xs, val_ts, train_norm_xs, train_ts, mu,batch_size,max_iters)\n",
        "plt.plot(range(0,max_iters,40),acc_list,\"r-\")\n",
        "plt.title(\"classification accuracy per iteration; $\\mu$=\"+str(mu)+\" batch size=\"+str(batch_size))\n",
        "\n",
        "\n",
        "#serch for optimality\n",
        "\n",
        "# best_mu = 0\n",
        "# best_acc = 0\n",
        "# while mu < 1.5:\n",
        "#   test_acc = 0\n",
        "#   mu = mu + 0.001\n",
        "#   for i in range(0,3):\n",
        "#       opt_w,opt_b,cost_list,acc_list = run_gradient_descent(w0,b0, val_norm_xs, val_ts, train_norm_xs, train_ts, mu,batch_size,max_iters)\n",
        "#       test_preds = ML_DL_Functions2.pred(opt_w, opt_b, test_norm_xs)\n",
        "#       test_acc = ML_DL_Functions2.get_accuracy(test_preds, test_ts) + test_acc\n",
        "#   if best_acc < test_acc/3:\n",
        "#       best_mu = mu\n",
        "#       best_acc = test_acc/3\n",
        "# print(best_mu)\n",
        "# print(best_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ5HlDFAzGrH"
      },
      "source": [
        "### 2.7 Finding and saving optimal values\n",
        "\n",
        "Find the optimal value of ${\\bf w}$ and $b$ in the means of accuracy using your code. Notice that the choice of $\\mu$ and the batch size are important for this. Run the code below to save these parameters to a file in your google drive directory. submit to the moodle (alongside this file and the functions file) both the\n",
        "\"assignment2_submission_optimal_weights.npy\" file and \"assignment2_submission_optimal_bias.npy\" file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "1dFOFSwgzGrI"
      },
      "outputs": [],
      "source": [
        "#change these to the optimal weights and biases, leave the name the same\n",
        "np.save(drive_path+\"assignment2_submission_optimal_weights.npy\",opt_w)\n",
        "np.save(drive_path+\"assignment2_submission_optimal_bias.npy\",opt_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KrQqSj2zGrI"
      },
      "source": [
        "### 2.8 Results\n",
        "\n",
        "Using the values of `w` and `b` from part 2.7, compute your training accuracy, validation accuracy,\n",
        "and test accuracy. Are there any differences between those three values? If so, why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuKw2mLozGrI",
        "outputId": "b9831b7b-ba90-4428-be23-c7a11a6a4c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.733911025706102  val_acc =  0.733  test_acc =  0.7291111111111112\n"
          ]
        }
      ],
      "source": [
        "w = np.load(drive_path+\"assignment2_submission_optimal_weights.npy\")\n",
        "b = np.load(drive_path+\"assignment2_submission_optimal_bias.npy\")\n",
        "\n",
        "train_preds = ML_DL_Functions2.pred(w, b, train_norm_xs)\n",
        "val_preds = ML_DL_Functions2.pred(w, b, val_norm_xs)\n",
        "test_preds = ML_DL_Functions2.pred(w, b, test_norm_xs)\n",
        "\n",
        "train_acc = ML_DL_Functions2.get_accuracy(train_preds, train_ts)\n",
        "val_acc = ML_DL_Functions2.get_accuracy(val_preds, val_ts)\n",
        "test_acc = ML_DL_Functions2.get_accuracy(test_preds, test_ts)\n",
        "\n",
        "\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO-V4yYACAtF"
      },
      "source": [
        "### 2.9 Using Pytorch\n",
        "Writing a classifier like this is instructive, and helps you understand what happens when\n",
        "we train a model. However, in practice, we rarely write model building and training code\n",
        "from scratch. Instead, we typically use one of the well-tested libraries available in a package. The following example showes you how this task could have been achieved using the deep learning library, pytorch. The library greatly simplifies the steps needed to create a learning model. Though there is nothing you need to complete in this section we suggest you read this section thoroughly and make sure you understand all the code. In the next assignment you will need to build a deep learning model yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STfBYRNUGESO"
      },
      "source": [
        "The first step required to use the pytorch module is to create a class which will be our model. in this case we will use a linear layer with a costum size(in your assignment you used a 90,1 linear layer meaning an input size of 90 and an output size of 1). We also add a sigmoid function to restrict the values between 0 and 1.\n",
        "\n",
        "The forward function is called everytime you call the model by name. It is equivalent to the prediction function you wrote but it serves another purpose since it saves all the operations done to the tensor which can then be used to calculate the gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "nFsHov0TCYzU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class single_layer(torch.nn.Module):\n",
        "  def __init__(self,input_size,output_size):\n",
        "    super(single_layer,self).__init__()\n",
        "    self.neuron = torch.nn.Linear(input_size,output_size)\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "  def forward(self,X):\n",
        "    out = self.neuron(X)\n",
        "    out = self.sigmoid(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "#my add on. I tried playing with pytorch by adding some layers for fun and testing the result\n",
        "class MultiLayerModel(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(MultiLayerModel, self).__init__()\n",
        "    self.activation = torch.nn.Sigmoid()\n",
        "    self.layer1 = torch.nn.Linear(input_size, hidden_size)\n",
        "    self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "    self.layer3 = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, X):\n",
        "    out = self.layer1(X)\n",
        "    out = self.activation(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.activation(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.activation(out)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD7R_h_DHHxl"
      },
      "source": [
        "We can now create a new model.\n",
        "\n",
        "we don't have to write the binary cross entropy loss since it is already written for us(criterion).\n",
        "\n",
        "Also instead of writing the optimzation proccess which in our case was gradient descent(W[n+1] = w[n]-$\\mu$dL/dW) we can use a pre built optimizer(SGD)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "tB3DPFRfHB8s"
      },
      "outputs": [],
      "source": [
        "model = single_layer(90,1)\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.05)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lVhHsxKISRt"
      },
      "source": [
        "There are a few pre-built training functions but usually the training function is written by hand. this function is similar to the one you wrote in this assignment only we now can use the pre-built tensor functions. Make sure you understood all the differences between the two:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "goyjkL11D5OA"
      },
      "outputs": [],
      "source": [
        "import pdb\n",
        "def train_model(model, criterion, optimizer, batch_size=100, max_iters=100):\n",
        "  iter = 0\n",
        "  cost_list = []\n",
        "  acc_list  = []\n",
        "  train_norm_xs_shuff = train_norm_xs\n",
        "  train_ts_shuff = train_ts\n",
        "  val_X_tensor = torch.tensor(val_norm_xs,dtype=torch.float32)\n",
        "  while iter < max_iters:\n",
        "    # shuffle the training set (there is code above for how to do this)\n",
        "    reindex = np.random.permutation(len(train_norm_xs))\n",
        "    train_norm_xs_shuff = train_norm_xs_shuff[reindex]\n",
        "    train_ts_shuff = train_ts_shuff[reindex]\n",
        "\n",
        "    for i in range(0, len(train_norm_xs), batch_size): # iterate over each minibatch\n",
        "      # minibatch that we are working with:\n",
        "      X = train_norm_xs_shuff[i:(i + batch_size)]\n",
        "      t = train_ts_shuff[i:(i + batch_size), 0]\n",
        "\n",
        "      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n",
        "      # the \"last\" minibatch\n",
        "      if np.shape(X)[0] != batch_size:\n",
        "        continue\n",
        "      # change the numpy types into torches tensors\n",
        "      X_tensor = torch.tensor(X,dtype=torch.float32)\n",
        "      t_tensor = torch.tensor(t,dtype=torch.float32).unsqueeze(1) # the unsqueeze reshapes (N,) to (N,1)\n",
        "\n",
        "      # a clean up step for PyTorch\n",
        "      optimizer.zero_grad()\n",
        "      # compute the prediction\n",
        "      prediction = model(X_tensor)\n",
        "      # compute the cost/loss\n",
        "      loss = criterion(prediction,t_tensor)\n",
        "      # calculate gradient(backpropegate)\n",
        "      loss.backward()\n",
        "      # update w and b(step)\n",
        "      optimizer.step()\n",
        "      # increment the iteration count\n",
        "      iter += 1\n",
        "      # compute and print the *validation* accuracy\n",
        "      if (iter % 40 == 0):\n",
        "        val_pred = model(val_X_tensor)\n",
        "        val_acc = ML_DL_Functions2.get_accuracy(val_pred,val_ts)\n",
        "        acc_list.append(val_acc)\n",
        "\n",
        "        print(\"Iter %d. [Val Acc %.1f%%]\" % (\n",
        "                iter, val_acc * 100))\n",
        "\n",
        "      if iter >= max_iters:\n",
        "        break\n",
        "\n",
        "\n",
        "  return acc_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XTEVQkgJu0i"
      },
      "source": [
        "We can now run the training proccess. You should get pretty similar results to the ones from the model you wrote. Make sure that the results are in the same range and if not fix your model and try again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ajs1myYKl4t",
        "outputId": "42330c65-82c8-4328-d3b8-02ff47b0954a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 40. [Val Acc 67.6%]\n",
            "Iter 80. [Val Acc 69.8%]\n",
            "Iter 120. [Val Acc 70.5%]\n",
            "Iter 160. [Val Acc 71.3%]\n",
            "Iter 200. [Val Acc 71.6%]\n",
            "Iter 240. [Val Acc 72.0%]\n",
            "Iter 280. [Val Acc 72.0%]\n",
            "Iter 320. [Val Acc 72.1%]\n",
            "Iter 360. [Val Acc 72.5%]\n",
            "Iter 400. [Val Acc 72.2%]\n",
            "Iter 440. [Val Acc 72.8%]\n",
            "Iter 480. [Val Acc 72.5%]\n",
            "Iter 40. [Val Acc 55.7%]\n",
            "Iter 80. [Val Acc 58.8%]\n",
            "Iter 120. [Val Acc 58.6%]\n",
            "Iter 160. [Val Acc 67.1%]\n",
            "Iter 200. [Val Acc 67.8%]\n",
            "Iter 240. [Val Acc 69.6%]\n",
            "Iter 280. [Val Acc 70.2%]\n",
            "Iter 320. [Val Acc 70.7%]\n",
            "Iter 360. [Val Acc 71.3%]\n",
            "Iter 400. [Val Acc 71.3%]\n",
            "Iter 440. [Val Acc 71.9%]\n",
            "Iter 480. [Val Acc 72.2%]\n",
            "Iter 520. [Val Acc 72.1%]\n",
            "Iter 560. [Val Acc 71.8%]\n",
            "Iter 600. [Val Acc 72.6%]\n",
            "Iter 640. [Val Acc 72.9%]\n",
            "Iter 680. [Val Acc 73.0%]\n",
            "Iter 720. [Val Acc 72.9%]\n",
            "Iter 760. [Val Acc 73.0%]\n",
            "Iter 800. [Val Acc 73.3%]\n",
            "Iter 840. [Val Acc 73.3%]\n",
            "Iter 880. [Val Acc 72.9%]\n",
            "Iter 920. [Val Acc 73.5%]\n",
            "Iter 960. [Val Acc 73.4%]\n",
            "Iter 1000. [Val Acc 73.3%]\n"
          ]
        }
      ],
      "source": [
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "#in the one layer result we get similare values to what we did earlier with the pred function (which makes sense since they are both one layered)\n",
        "acc_list = train_model(model,criterion,optimizer,100,500)\n",
        "\n",
        "#my add on. as we can see we get better results than one layer\n",
        "model = MultiLayerModel(90, 64, 1)\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "acc_list = train_model(model, criterion, optimizer, batch_size=50, max_iters=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFj6EzWAB7ry"
      },
      "source": [
        "You can also try to change the model(add layers or change layers) change the optimizer or the hyperparameters and try to improve the validation accuracy. If you want a challenge you can try reach a validation accuracy of 75%"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}